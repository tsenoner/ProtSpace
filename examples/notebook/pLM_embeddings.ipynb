{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pyfaidx import Fasta\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, EsmModel, T5EncoderModel, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoints\n",
    "Ankhs = [\n",
    "    \"ElnaggarLab/ankh-base\",\n",
    "    \"ElnaggarLab/ankh-large\",\n",
    "]\n",
    "\n",
    "ESMs = [\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    \"facebook/esm2_t30_150M_UR50D\",\n",
    "    \"facebook/esm2_t33_650M_UR50D\",\n",
    "    \"facebook/esm2_t36_3B_UR50D\",\n",
    "]\n",
    "\n",
    "Rostlab = [\n",
    "    \"Rostlab/prot_t5_xl_half_uniref50-enc\",\n",
    "    \"Rostlab/prot_t5_xl_uniref50\",\n",
    "    \"Rostlab/ProstT5_fp16\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fasta(fasta_path: Path, max_len=1022):\n",
    "    \"\"\"Remove sequences longer than max_len and save their identifiers to a file.\"\"\"\n",
    "    filtered_fasta_path = fasta_path.with_suffix(\".filtered.fasta\")\n",
    "    long_sequences_path = fasta_path.with_suffix(\".long_sequences.txt\")\n",
    "\n",
    "    with filtered_fasta_path.open(\n",
    "        \"w\"\n",
    "    ) as filtered_fasta, long_sequences_path.open(\"w\") as long_sequences:\n",
    "        for header, seq in Fasta(str(fasta_path)).items():\n",
    "            if len(seq) > max_len:\n",
    "                long_sequences.write(f\"{header.split()[0]}\\n\")\n",
    "            else:\n",
    "                filtered_fasta.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "    return filtered_fasta_path\n",
    "\n",
    "\n",
    "def seq_preprocess(df, model_type=\"esm\"):\n",
    "    df[\"sequence\"] = df[\"sequence\"].str.replace(\"[UZO]\", \"X\", regex=True)\n",
    "\n",
    "    if model_type in \"esm\":\n",
    "        return df\n",
    "    elif model_type == \"ankh\":\n",
    "        return df\n",
    "    elif model_type == \"pt\":\n",
    "        df[\"sequence\"] = df.apply(lambda row: \" \".join(row[\"sequence\"]), axis=1)\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_fasta(file_path: Path):\n",
    "    headers = []\n",
    "    sequences = []\n",
    "    fasta = Fasta(str(file_path))\n",
    "    for seq in fasta:\n",
    "        headers.append(seq.name)\n",
    "        sequences.append(str(seq))\n",
    "    return headers, sequences\n",
    "\n",
    "def parse_fasta(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                sequences.append([line[1:], ''])\n",
    "            elif sequences:\n",
    "                sequences[-1][1] += line\n",
    "\n",
    "    return dict(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(checkpoint):\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if \"esm\" in checkpoint:\n",
    "        mod_type = \"esm\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        model = EsmModel.from_pretrained(checkpoint)\n",
    "        model = model.to(device)\n",
    "    elif \"ankh\" in checkpoint:\n",
    "        mod_type = \"ankh\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        model = T5EncoderModel.from_pretrained(checkpoint)\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        mod_type = \"pt\"\n",
    "        tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "        model = T5EncoderModel.from_pretrained(\n",
    "            checkpoint, torch_dtype=torch.float16\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        model = model.half()\n",
    "\n",
    "    return model, tokenizer, mod_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(\n",
    "    checkpoint,\n",
    "    df,\n",
    "    emb_type=\"per_prot\",\n",
    "    output_file: Path = Path(\"protein_embeddings.h5\"),\n",
    "):\n",
    "    model, tokenizer, mod_type = setup_model(checkpoint)\n",
    "    model.eval()\n",
    "    df = seq_preprocess(df, mod_type)\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def compute_embedding(sequence, emb_type):\n",
    "        inputs = tokenizer(\n",
    "            sequence,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=10_000,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            add_special_tokens=True,\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).last_hidden_state.cpu().numpy()\n",
    "        if emb_type == \"per_res\":\n",
    "            # remove special tokens\n",
    "            if mod_type in [\"pt\", \"ankh\"]:\n",
    "                outputs = outputs[:-1, :]\n",
    "            elif mod_type == \"esm\":\n",
    "                outputs = np.squeeze(outputs, axis=0)[:-1, :]\n",
    "            return outputs\n",
    "        elif emb_type == \"per_prot\":\n",
    "            return outputs.mean(axis=1).flatten()\n",
    "        else:\n",
    "            raise ValueError(\"Input valid embedding type\")\n",
    "\n",
    "    # Open the HDF file in append mode\n",
    "    with h5py.File(output_file, \"a\") as hdf:\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            sequence = row[\"sequence\"]\n",
    "            header = row[\"header\"]\n",
    "\n",
    "            # Check if the embedding already exists\n",
    "            if header in hdf:\n",
    "                continue\n",
    "\n",
    "            embedding = compute_embedding(sequence, emb_type)\n",
    "            hdf.create_dataset(name=header, data=embedding)\n",
    "\n",
    "    # clean up gpu\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del df\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables / parameters\n",
    "fasta_file = \"<path2fasta>\"\n",
    "fasta_file = Path(fasta_file)\n",
    "max_seq_len = None\n",
    "model_checkpoint = Rostlab[0]\n",
    "emb_type = \"per_prot\"  # per_prot, per_res\n",
    "\n",
    "\n",
    "\n",
    "if max_seq_len is not None:\n",
    "    filtered_fasta_path = process_fasta(fasta_file, max_seq_len)\n",
    "    headers, sequences = read_fasta(filtered_fasta_path)\n",
    "else:\n",
    "    headers, sequences = read_fasta(fasta_file)\n",
    "\n",
    "df = pd.DataFrame({\"header\": headers, \"sequence\": sequences})\n",
    "\n",
    "output_file = fasta_file.with_suffix(\".h5\")\n",
    "\n",
    "print(f\"Embeddings out: {output_file}\")\n",
    "create_embedding(\n",
    "    model_checkpoint,\n",
    "    df,\n",
    "    emb_type=emb_type,\n",
    "    output_file=output_file,\n",
    ")\n",
    "print(f\"Embeddings saved to {output_file}\")\n",
    "\n",
    "# Remove the temporary filtered FASTA file if it was created\n",
    "if max_seq_len is not None:\n",
    "    filtered_fasta_path.unlink()\n",
    "    print(f\"Temporary filtered FASTA file removed: {filtered_fasta_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "into-the-unknown-NYQelt9P-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
